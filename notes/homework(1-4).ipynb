{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 4-1"
      ],
      "metadata": {
        "id": "FBeS_VeonBOJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import h5py\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import scipy\n",
        "from PIL import Image\n",
        "from scipy import ndimage\n",
        "import tensorflow.keras.datasets.cifar10 as cif"
      ],
      "metadata": {
        "id": "x_ZwKvP5vnol"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid_backward(dA, cache):\n",
        "    \"\"\"\n",
        "    The backward propagation for a single SIGMOID unit.\n",
        "    Arguments:\n",
        "    dA - post-activation gradient, of any shape\n",
        "    cache - 'Z' where we store for computing backward propagation efficiently\n",
        "    Returns:\n",
        "    dZ - Gradient of the cost with respect to Z\n",
        "    \"\"\"\n",
        "    Z = cache \n",
        "    s = 1/(1+np.exp(-Z))\n",
        "    dZ = dA * s * (1-s)\n",
        "    return dZ\n",
        "\n",
        "def relu_backward(dA, cache):\n",
        "    \"\"\"\n",
        "    The backward propagation for a single RELU unit.\n",
        "    Arguments:\n",
        "    dA - post-activation gradient, of any shape\n",
        "    cache - 'Z' where we store for computing backward propagation efficiently\n",
        "    Returns:\n",
        "    dZ - Gradient of the cost with respect to Z\n",
        "    \"\"\"\n",
        "    Z = cache\n",
        "    # just converting dz to a correct object.\n",
        "    dZ = np.array(dA, copy=True)\n",
        "    # When z <= 0, we should set dz to 0 as well. \n",
        "    dZ[Z <= 0] = 0\n",
        "    return dZ\n",
        "\n",
        "def sigmoid(Z):\n",
        "    \"\"\"\n",
        "    Numpy sigmoid activation implementation\n",
        "    Arguments:\n",
        "    Z - numpy array of any shape\n",
        "    Returns:\n",
        "    A - output of sigmoid(z), same shape as Z\n",
        "    cache - returns Z as well, useful during backpropagation\n",
        "    \"\"\"\n",
        "    A = 1/(1+np.exp(-Z))\n",
        "    cache = Z\n",
        "    return A, cache\n",
        "\n",
        "def relu(Z):\n",
        "    \"\"\"\n",
        "    Numpy Relu activation implementation\n",
        "    Arguments:\n",
        "    Z - Output of the linear layer, of any shape\n",
        "    Returns:\n",
        "    A - Post-activation parameter, of the same shape as Z\n",
        "    cache - a python dictionary containing \"A\"; stored for computing the backward pass efficiently\n",
        "    \"\"\"\n",
        "    A = np.maximum(0,Z)  \n",
        "    cache = Z \n",
        "    return A, cache\n",
        "\n",
        "def initialize_parameters(n_x, n_h, n_y):\n",
        "    np.random.seed(1)\n",
        "\n",
        "    W1 = np.random.randn(n_h, n_x)*0.01\n",
        "    b1 = np.zeros((n_h,1))\n",
        "    W2 = np.random.randn(n_y, n_h)*0.01\n",
        "    b2 = np.zeros((n_y,1))\n",
        "\n",
        "    assert(W1.shape == (n_h, n_x))\n",
        "    assert(b1.shape == (n_h, 1))\n",
        "    assert(W2.shape == (n_y, n_h))\n",
        "    assert(b2.shape == (n_y, 1))\n",
        "    \n",
        "    parameters = {\"W1\": W1,\n",
        "                  \"b1\": b1,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2}\n",
        "    \n",
        "    return parameters    \n",
        "\n",
        "def initialize_parameters_deep(layer_dims):\n",
        "    np.random.seed(3)\n",
        "    parameters = {}\n",
        "    L = len(layer_dims)            # number of layers in the network\n",
        "\n",
        "    for l in range(1, L):\n",
        "        ### START CODE HERE ### (≈ 2 lines of code)\n",
        "        parameters['W' + str(l)] = np.random.randn(layer_dims[l],layer_dims[l-1])*0.01\n",
        "        parameters['b' + str(l)] = np.zeros((layer_dims[l],1))\n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
        "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
        "\n",
        "        \n",
        "    return parameters\n",
        "\n",
        "## initialize end\n",
        "\n",
        "def linear_forward(A, W, b):\n",
        "    Z = np.dot(W,A) + b\n",
        "    \n",
        "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
        "    cache = (A, W, b)\n",
        "    \n",
        "    return Z, cache\n",
        "\n",
        "def linear_activation_forward(A_prev, W, b, activation):\n",
        "    if activation == \"sigmoid\":\n",
        "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
        "        Z, linear_cache = linear_forward(A_prev,W,b)\n",
        "        A, activation_cache = sigmoid(Z)\n",
        "    \n",
        "    elif activation == \"relu\":\n",
        "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
        "        Z, linear_cache = linear_forward(A_prev,W,b)\n",
        "        A, activation_cache = relu(Z)\n",
        "    \n",
        "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
        "    cache = (linear_cache, activation_cache)\n",
        "\n",
        "    return A, cache\n",
        "\n",
        "def L_model_forward(X, parameters):\n",
        "    caches = []\n",
        "    A = X\n",
        "    L = len(parameters) // 2                  # number of layers in the neural network\n",
        "    \n",
        "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
        "    for l in range(1, L):\n",
        "        A_prev = A \n",
        "        A, cache = linear_activation_forward(A_prev,parameters['W' + str(l)],parameters['b' + str(l)],activation = \"relu\")\n",
        "        caches.append(cache)\n",
        "    \n",
        "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
        "    ### START CODE HERE ### (≈ 2 lines of code)\n",
        "    AL, cache = linear_activation_forward(A,parameters['W' + str(L)],parameters['b' + str(L)],activation = \"sigmoid\")\n",
        "    caches.append(cache)\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    assert(AL.shape == (1,X.shape[1]))\n",
        "            \n",
        "    return AL, caches\n",
        "\n",
        "def compute_cost(AL, Y):\n",
        "    \"\"\"\n",
        "    Implement the cost function defined by equation (7).\n",
        "\n",
        "    Arguments:\n",
        "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
        "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
        "\n",
        "    Returns:\n",
        "    cost -- cross-entropy cost\n",
        "    \"\"\"\n",
        "    \n",
        "    m = Y.shape[1]\n",
        "\n",
        "    # Compute loss from aL and y.\n",
        "    ### START CODE HERE ### (≈ 1 lines of code)\n",
        "    cost = -1 / m * np.sum(Y * np.log(AL) + (1-Y) * np.log(1-AL),axis=1,keepdims=True)\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
        "    assert(cost.shape == ())\n",
        "    \n",
        "    return cost\n",
        "\n",
        "def linear_backward(dZ, cache):\n",
        "    \"\"\"\n",
        "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
        "\n",
        "    Arguments:\n",
        "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
        "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
        "\n",
        "    Returns:\n",
        "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
        "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
        "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
        "    \"\"\"\n",
        "    A_prev, W, b = cache\n",
        "    m = A_prev.shape[1]\n",
        "   ### START CODE HERE ### (≈ 3 lines of code)\n",
        "    dW = 1 / m * np.dot(dZ ,A_prev.T)\n",
        "    db = 1 / m * np.sum(dZ,axis = 1 ,keepdims=True)\n",
        "    dA_prev = np.dot(W.T,dZ) \n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    assert (dA_prev.shape == A_prev.shape)\n",
        "    assert (dW.shape == W.shape)\n",
        "    assert (db.shape == b.shape)\n",
        "    \n",
        "    return dA_prev, dW, db\n",
        "\n",
        "def linear_activation_backward(dA, cache, activation):\n",
        "    linear_cache, activation_cache = cache\n",
        "    \n",
        "    if activation == \"relu\":\n",
        "        ### START CODE HERE ### (≈ 2 lines of code)\n",
        "        dZ = relu_backward(dA, activation_cache)\n",
        "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "    elif activation == \"sigmoid\":\n",
        "        ### START CODE HERE ### (≈ 2 lines of code)\n",
        "        dZ = sigmoid_backward(dA, activation_cache)\n",
        "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "        ### END CODE HERE ###\n",
        "    \n",
        "    return dA_prev, dW, db\n",
        "\n",
        "def L_model_backward(AL, Y, caches):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
        "    \n",
        "    Arguments:\n",
        "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
        "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
        "    caches -- list of caches containing:\n",
        "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
        "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
        "    \n",
        "    Returns:\n",
        "    grads -- A dictionary with the gradients\n",
        "             grads[\"dA\" + str(l)] = ...\n",
        "             grads[\"dW\" + str(l)] = ...\n",
        "             grads[\"db\" + str(l)] = ...\n",
        "    \"\"\"\n",
        "    grads = {}\n",
        "    L = len(caches) # the number of layers\n",
        "    m = AL.shape[1]\n",
        "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
        "\n",
        "    # Initializing the backpropagation\n",
        "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
        "    \n",
        "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"AL, Y, caches\". Outputs: \"grads[\"dAL\"], grads[\"dWL\"], grads[\"dbL\"]\n",
        "    current_cache = caches[L-1]\n",
        "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation = \"sigmoid\")\n",
        "      \n",
        "    for l in reversed(range(L - 1)):\n",
        "        # lth layer: (RELU -> LINEAR) gradients.\n",
        "        # Inputs: \"grads[\"dA\" + str(l + 2)], caches\". Outputs: \"grads[\"dA\" + str(l + 1)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
        "        current_cache = caches[l]\n",
        "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l+2)], current_cache, activation = \"relu\")\n",
        "        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n",
        "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
        "        grads[\"db\" + str(l + 1)] = db_temp\n",
        "\n",
        "    return grads\n",
        "\n",
        "def update_parameters(parameters, grads, learning_rate):\n",
        "    \n",
        "    L = len(parameters) // 2 # number of layers in the neural network\n",
        "\n",
        "    # Update rule for each parameter. Use a for loop.\n",
        "    for l in range(L):\n",
        "        parameters[\"W\" + str(l+1)] =  parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n",
        "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l + 1)]\n",
        "        \n",
        "    return parameters"
      ],
      "metadata": {
        "id": "ONGXMuw6tYXW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4-2"
      ],
      "metadata": {
        "id": "N1MxMfGAnGFz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "# %load_ext autoreload\n",
        "# %autoreload 2\n",
        "\n",
        "np.random.seed(1)\n",
        "\n",
        "(train_x_orig, train_y), (test_x_orig, test_y) = cif.load_data()\n",
        "\n",
        "for i in range(0, train_x_orig.shape[0]):\n",
        "  if train_y[i] == 3:\n",
        "    train_y[i] = 1\n",
        "  else:\n",
        "    train_y[i] = 0\n",
        "\n",
        "for i in range(0, test_x_orig.shape[0]):\n",
        "  if test_y[i] == 3:\n",
        "    test_y[i] = 1\n",
        "  else:\n",
        "    test_y[i] = 0\n",
        "\n",
        "\n",
        "train_y = train_y.T\n",
        "test_y = test_y.T\n",
        "\n",
        "# # Example of a picture\n",
        "index = 9\n",
        "plt.imshow(train_x_orig[index])\n",
        "print (\"y = \" + str(train_y[0, index]))\n",
        "\n",
        "# Explore your dataset \n",
        "m_train = train_x_orig.shape[0]\n",
        "num_px = train_x_orig.shape[1]\n",
        "m_test = test_x_orig.shape[0]\n",
        "\n",
        "# print (\"Number of training examples: \" + str(m_train))\n",
        "# print (\"Number of testing examples: \" + str(m_test))\n",
        "print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
        "# print (\"train_x_orig shape: \" + str(train_x_orig.shape))\n",
        "# print (\"train_y shape: \" + str(train_y.shape))\n",
        "# print (\"test_x_orig shape: \" + str(test_x_orig.shape))\n",
        "# print (\"test_y shape: \" + str(test_y.shape))\n",
        "\n",
        "# Reshape the training and test examples \n",
        "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\n",
        "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
        "\n",
        "# Standardize data to have feature values between 0 and 1.\n",
        "train_x = train_x_flatten/255.\n",
        "test_x = test_x_flatten/255.\n",
        "\n",
        "print (\"train_x's shape: \" + str(train_x.shape))\n",
        "print (\"test_x's shape: \" + str(test_x.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "id": "DQAtYClGQTYK",
        "outputId": "20068639-6b4e-4837-d061-d4e8a4f5de94"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 3s 0us/step\n",
            "y = 1\n",
            "Each image is of size: (32, 32, 3)\n",
            "train_x's shape: (3072, 50000)\n",
            "test_x's shape: (3072, 10000)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### CONSTANTS DEFINING THE MODEL ####\n",
        "n_x = 3072     # num_px * num_px * 3\n",
        "n_h = 7\n",
        "n_y = 1\n",
        "layers_dims = (n_x, n_h, n_y)\n",
        "# GRADED FUNCTION: two_layer_model\n",
        "\n",
        "def two_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
        "    \"\"\"\n",
        "    Implements a two-layer neural network: LINEAR->RELU->LINEAR->SIGMOID.\n",
        "    \n",
        "    Arguments:\n",
        "    X -- input data, of shape (n_x, number of examples)\n",
        "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
        "    layers_dims -- dimensions of the layers (n_x, n_h, n_y)\n",
        "    num_iterations -- number of iterations of the optimization loop\n",
        "    learning_rate -- learning rate of the gradient descent update rule\n",
        "    print_cost -- If set to True, this will print the cost every 100 iterations \n",
        "    \n",
        "    Returns:\n",
        "    parameters -- a dictionary containing W1, W2, b1, and b2\n",
        "    \"\"\"\n",
        "    \n",
        "    np.random.seed(1)\n",
        "    grads = {}\n",
        "    costs = []                              # to keep track of the cost\n",
        "    m = X.shape[1]                           # number of examples\n",
        "    (n_x, n_h, n_y) = layers_dims\n",
        "    \n",
        "    # Initialize parameters dictionary, by calling one of the functions you'd previously implemented\n",
        "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
        "\n",
        "    # Get W1, b1, W2 and b2 from the dictionary parameters.\n",
        "    W1 = parameters[\"W1\"]\n",
        "    b1 = parameters[\"b1\"]\n",
        "    W2 = parameters[\"W2\"]\n",
        "    b2 = parameters[\"b2\"]\n",
        "    \n",
        "    # Loop (gradient descent)\n",
        "\n",
        "    for i in range(0, num_iterations):\n",
        "\n",
        "        # Forward propagation: LINEAR -> RELU -> LINEAR -> SIGMOID. Inputs: \"X, W1, b1\". Output: \"A1, cache1, A2, cache2\".\n",
        "        A1, cache1 =linear_activation_forward(X, W1, b1, activation = \"relu\")\n",
        "        A2, cache2 = linear_activation_forward(A1, W2, b2, activation = \"sigmoid\")\n",
        "        \n",
        "        # Compute cost\n",
        "        cost = compute_cost(A2, Y)\n",
        "        \n",
        "        # Initializing backward propagation\n",
        "        dA2 = - (np.divide(Y, A2) - np.divide(1 - Y, 1 - A2))\n",
        "        \n",
        "        # Backward propagation. Inputs: \"dA2, cache2, cache1\". Outputs: \"dA1, dW2, db2; also dA0 (not used), dW1, db1\".\n",
        "        dA1, dW2, db2 = linear_activation_backward(dA2, cache2, activation = \"sigmoid\")\n",
        "        dA0, dW1, db1 = linear_activation_backward(dA1, cache1, activation = \"relu\")\n",
        "\n",
        "        # Set grads['dWl'] to dW1, grads['db1'] to db1, grads['dW2'] to dW2, grads['db2'] to db2\n",
        "        grads['dW1'] = dW1\n",
        "        grads['db1'] = db1\n",
        "        grads['dW2'] = dW2\n",
        "        grads['db2'] = db2\n",
        "        \n",
        "        # Update parameters.\n",
        "        parameters = update_parameters(parameters, grads, learning_rate)\n",
        "\n",
        "        # Retrieve W1, b1, W2, b2 from parameters\n",
        "        W1 = parameters[\"W1\"]\n",
        "        b1 = parameters[\"b1\"]\n",
        "        W2 = parameters[\"W2\"]\n",
        "        b2 = parameters[\"b2\"]\n",
        "        \n",
        "        # Print the cost every 100 training example\n",
        "        if print_cost and i % 100 == 0:\n",
        "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
        "        if print_cost and i % 100 == 0:\n",
        "            costs.append(cost)\n",
        "       \n",
        "    # plot the cost\n",
        "\n",
        "    plt.plot(np.squeeze(costs))\n",
        "    plt.ylabel('cost')\n",
        "    plt.xlabel('iterations (per tens)')\n",
        "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
        "    plt.show()\n",
        "    \n",
        "    return parameters\n",
        "  \n",
        "parameters = two_layer_model(train_x, train_y, layers_dims = (n_x, n_h, n_y), num_iterations = 2500, print_cost=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 754
        },
        "id": "kj3DNUldtbpY",
        "outputId": "a99ee3f6-013b-4b61-d427-850fa0f27a0c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cost after iteration 0: 0.6911251019068702\n",
            "Cost after iteration 100: 0.32904420011874325\n",
            "Cost after iteration 200: 0.32747437935959683\n",
            "Cost after iteration 300: 0.3260642422063041\n",
            "Cost after iteration 400: 0.3247558995033161\n",
            "Cost after iteration 500: 0.32352290445755605\n",
            "Cost after iteration 600: 0.3223484679063313\n",
            "Cost after iteration 700: 0.3212221676002741\n",
            "Cost after iteration 800: 0.3201379702327775\n",
            "Cost after iteration 900: 0.31909157403890576\n",
            "Cost after iteration 1000: 0.31808027169648756\n",
            "Cost after iteration 1100: 0.31710151976872913\n",
            "Cost after iteration 1200: 0.31615167017494855\n",
            "Cost after iteration 1300: 0.3152275652680054\n",
            "Cost after iteration 1400: 0.3143288971314132\n",
            "Cost after iteration 1500: 0.3134539007417701\n",
            "Cost after iteration 1600: 0.31259947368649\n",
            "Cost after iteration 1700: 0.31176390882178884\n",
            "Cost after iteration 1800: 0.31094472347728225\n",
            "Cost after iteration 1900: 0.3101425730240895\n",
            "Cost after iteration 2000: 0.3093557906443586\n",
            "Cost after iteration 2100: 0.3085885619773915\n",
            "Cost after iteration 2200: 0.3078426557389774\n",
            "Cost after iteration 2300: 0.30711775977572736\n",
            "Cost after iteration 2400: 0.3064182845377778\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}
